{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - read data from csv. \n",
    "We use the pandas built-in function <br>\n",
    "Data must also be standardized to avoid computational errors\n",
    "\n",
    "`data=(data-data.mean())/data.std() # standatization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_real_dataset(dims_3 = False):\n",
    "\n",
    "    ''' read real dataset from home_data.csv '''\n",
    "\n",
    "    #             y             x1\n",
    "    columns = ['price', 'sqft_living']\n",
    "\n",
    "    if dims_3:     #x2   \n",
    "        columns += ['sqft_above']\n",
    "\n",
    "    data = pd.read_csv('./dataset/home_data.csv').loc[:, columns]\n",
    "    # data.hist(bins=150) # check dataset distribution\n",
    "    data=(data-data.mean())/data.std() # standartization\n",
    "    x, y = data.iloc[:, 1:].to_numpy(), data.iloc[:,0].to_numpy()\n",
    "    x_0 = np.ones(len(x))\n",
    "    x = np.column_stack((x_0, x))\n",
    "\n",
    "    #x_2 = np.random.normal(0, 1, x.shape[0])#np.square(x) * np.sin(x/10)\n",
    "    #x = np.column_stack((x_0, x, x_2))\n",
    "    \n",
    "    x_test, y_test = x[20000:], y[20000:]\n",
    "\n",
    "    return x, y, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - define predict function, cost function and function that count a gradient of cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, weights): \n",
    "    return np.matmul(x, weights)\n",
    "\n",
    "def cost_func(y, y_hat):\n",
    "    return np.mean(np.square(y_hat - y))\n",
    "\n",
    "def count_gradient(x, weights, y):\n",
    "    vec_w = np.zeros(weights.shape)\n",
    "   \n",
    "    for i, weight in enumerate(weights):\n",
    "        y_hat = predict(x, weights)\n",
    "        x_i = x[:, i]\n",
    "        y_diff = y_hat - y\n",
    "        vec_w[i] = np.dot(y_diff, x_i) # count partial derivative\n",
    "   \n",
    "    return vec_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Gradient descent algorithm\n",
    "\n",
    "Now we can define an algorithm <br>\n",
    "NOTE: `epoch_results = []` is not a part of the algo itself. Is is used for visualization of all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_algorithm(x, weights, y, epochs, lr, regular_param, display_cost = False):\n",
    "    epoch_results = [] #for visualisation\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        y_hat = predict(x, weights)\n",
    "\n",
    "        epoch_results.append({'y_hat': y_hat, 'w': weights, 'e': epoch}) # save results\n",
    "        \n",
    "        if display_cost:\n",
    "            cost = cost_func(y, y_hat)\n",
    "            print(cost)\n",
    "\n",
    "        gradient = count_gradient(x, weights, y)\n",
    "\n",
    "        weights = weights - lr/len(x) * (gradient + regular_param * weights)\n",
    "    \n",
    "    return weights, epoch_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Prepare visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_function(x, y, plot = plt, as_line = False, color = 'blue'):\n",
    "    if as_line:\n",
    "        x = x[:, 1]\n",
    "        sort_args = np.argsort(x)\n",
    "        x = x[sort_args]\n",
    "        y = y[sort_args]\n",
    "        plot.plot(x, y, color = color)\n",
    "    else:\n",
    "        plot.scatter(x[:, 1],y, color = color, s=0.001)\n",
    "\n",
    "def visualize_gd_results(epoch_results, lr, x, y):\n",
    "    saved_epochs_n = len(epoch_results)\n",
    "\n",
    "    n_rows = saved_epochs_n // 4\n",
    "    n_cols = saved_epochs_n//n_rows\n",
    "\n",
    "    subplots = plt.subplots(n_rows, n_cols)\n",
    "    subplots[0].suptitle(f'Linear regression with Gradient Descent (lr = {lr})')\n",
    "    subplots = subplots[1].reshape(saved_epochs_n, )\n",
    "\n",
    "    for i, plot in enumerate(subplots):\n",
    "        draw_function(x, y, plot)\n",
    "        draw_function(x, epoch_results[i]['y_hat'], plot, True, 'red')\n",
    "        \n",
    "        counted_legend = f'w_1 = {\"{:.3f}\".format(epoch_results[i][\"w\"][1])}\\nw_0 = {\"{:.4f}\".format(epoch_results[i][\"w\"][0])}'\n",
    "        data_legend = 'Data'\n",
    "        \n",
    "        plot.legend([counted_legend, data_legend])\n",
    "        plot.set_title(label = f\"epoch = {epoch_results[i]['e']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - put it all together\n",
    "\n",
    "Drawing the plot of all epochs may take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, x_test, y_test = read_real_dataset() \n",
    "weights = np.random.normal(0, 1, x.shape[1]) # generate random weights ~ N(0, 1)\n",
    "\n",
    "epochs, learning_rate, regularization_param = 8, 0.2, 0.1\n",
    "\n",
    "weights, epoch_results = gradient_descent_algorithm(x, weights, y, \n",
    "                                                        epochs, learning_rate, regularization_param)\n",
    "\n",
    "y_hat = predict(x_test, weights)\n",
    "    \n",
    "print(\"w_1 (k) - {:.3f}\".format(weights[1]))\n",
    "print(\"w_0 (b) - {:.3f}\".format(weights[0]))\n",
    "\n",
    "visualize_gd_results(epoch_results, learning_rate, x, y)\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "draw_function(x, y, as_line=False)\n",
    "draw_function(x_test, y_hat, as_line=True, color='red')\n",
    "print(cost_func(y, predict(x, weights)))\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "draw_function(x_test, y_test, as_line=False)\n",
    "draw_function(x_test, y_hat, as_line=True, color='red')\n",
    "print(cost_func(y_test, y_hat))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}